{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Submission 1 Belajar Pengembangan Machine Learning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNJ5Bt/LnxEZqyVlgDRZZL8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"g25BwUJhJ0TL"},"source":["!pip install kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hge1bZyJ-En"},"source":["from google.colab import files\n","files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ylj3ktu5KAGg"},"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oU542TnkKMjH"},"source":["!kaggle datasets download -d arkhoshghalb/twitter-sentiment-analysis-hatred-speech"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zJYDhDgzKR2N"},"source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional,Flatten,Dropout\n","from keras.utils.np_utils import to_categorical\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tR7A64ukKTtd"},"source":["import zipfile\n","zip_ref = zipfile.ZipFile('twitter-sentiment-analysis-hatred-speech.zip', 'r')\n","zip_ref.extractall('files')\n","zip_ref.close()\n"," \n","data_frame = pd.read_csv('/content/files/train.csv')\n","data_frame"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3tA4Xr5KWzo"},"source":["negative = len(data_frame['label']=='0')/2\n","positive = len(data_frame['label']=='1')/2\n","sns.countplot(data_frame['label'])\n","print('Positive reviews are {},and negative reviews are {} of total {} '.format(positive,negative,len(data_frame)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJG05bhMKgua"},"source":["le = LabelEncoder()\n","training_reviews,testing_reviews,training_labels,testing_labels  = train_test_split(data_frame['tweet'].values,data_frame['label'].values,test_size = 0.2)\n","training_labels = le.fit_transform(training_labels)\n","testing_labels = le.fit_transform(testing_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTZFf0LsKkEP"},"source":["def hapus_spesial_char(tweets):\n","        special_numeric=\"\"\n","        for character in tweets:\n","            if character.isalpha() or character==\" \":\n","                special_numeric += character\n","        return special_numeric\n","def hapus_tag(text):\n","     return re.compile(r\"<[^>]+>#@ðº¦±©â¤ïªï;â&¬ë_µì°\").sub(\" \", text)\n","def hapus_no(text):\n","     return \"\".join(re.sub(r\"([0–9]+)\",\" \",text))\n","data_frame.tweet=data_frame.tweet.apply(lambda x : hapus_tag(x))\n","data_frame.tweet=data_frame.tweet.apply(lambda x : hapus_no(x))\n","data_frame.tweet=(data_frame.tweet).apply(hapus_spesial_char)\n","data_frame.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qI9C5ZACKkG3"},"source":["tokenizer = Tokenizer(num_words=10000,oov_token='<OOV>')\n","tokenizer.fit_on_texts(training_reviews)\n","word_index = tokenizer.word_index\n","vocab_size = len(word_index) + 1\n","training_sequence = tokenizer.texts_to_sequences(training_reviews)\n","testing_sequence = tokenizer.texts_to_sequences(testing_reviews)\n","train_pad_sequence = pad_sequences(training_sequence,maxlen = 500,truncating= 'post',padding = 'pre')\n","test_pad_sequence = pad_sequences(testing_sequence,maxlen = 500,truncating= 'post',padding = 'pre')\n","print('Total Unique Words : {}'.format(len(word_index)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DSpC7n39KpnI"},"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oadEUqK3K0xw"},"source":["!unzip glove*.zip\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQAaIbyyK2rZ"},"source":["embedded_words = {}\n","with open ('/content/glove.6B.200d.txt') as file:\n","  for line in file:\n","    words, coeff = line.split(maxsplit=1)\n","    coeff = np.array(coeff.split(),dtype = float)\n","    embedded_words[words] = coeff"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aqKnsXVtK2u0"},"source":["embedding_matrix = np.zeros((len(word_index) + 1,200))\n","for word, i in word_index.items():\n","  embedding_vector = embedded_words.get(word)\n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PfEtu9VVK6mH"},"source":["model = tf.keras.Sequential([tf.keras.layers.Embedding(len(word_index) + 1,200,weights=[embedding_matrix],input_length=500,\n","                            trainable=False),\n","                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n","                             tf.keras.layers.Dropout(0.5),\n","                             tf.keras.layers.Dense(256,activation = 'relu',),\n","                             tf.keras.layers.Dense(128,activation = 'relu'),\n","                             tf.keras.layers.Dropout(0.5),\n","                             tf.keras.layers.Dense(1,activation = tf.nn.sigmoid)])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIMre0K7K_TJ"},"source":["from keras.callbacks import EarlyStopping\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n","history = model.fit(train_pad_sequence,training_labels, epochs = 5, callbacks=[es], validation_data=(test_pad_sequence,testing_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0lFlpPYdLBgQ"},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'r', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend(loc=0)\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend(loc=0)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SdA04E9rLFYV"},"source":["print('Training Accuracy: {}'.format(max(acc)))\n","print('Validation Accuracy: {}'.format(max(val_acc)))"],"execution_count":null,"outputs":[]}]}